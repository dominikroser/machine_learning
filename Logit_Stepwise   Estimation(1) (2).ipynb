{
  "metadata": {
    "analyzedDataset": "wage",
    "creator": "admin",
    "kernelspec": {
      "name": "py-dku-venv-datascience",
      "display_name": "Python (env datascience)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "tags": [],
    "createdOn": 1651825386837,
    "customFields": {},
    "hide_input": false,
    "modifiedBy": "ajmendez@faculty.ie.edu"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## %pylab inline"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.metrics import roc_curve, auc\nimport pylab as pl\nimport numpy as np\n\nimport warnings                              # Disable some warnings\nwarnings.filterwarnings(\"ignore\",category\u003dDeprecationWarning)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read the dataset as a Pandas dataframe in memory\ndataset_wage \u003d dataiku.Dataset(\"Risk\")\ndf \u003d dataset_wage.get_dataframe(limit\u003d100000)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get some simple descriptive statistics\npdu.audit(df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stepwise function\ndef stepwise_selection(X, y, \n                       initial_list\u003d[], \n                       threshold_in\u003d0.05, \n                       threshold_out \u003d 0.1, \n                       verbose\u003dTrue):\n    \"\"\" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value \u003c threshold_in\n        threshold_out - exclude a feature if its p-value \u003e threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in \u003c threshold_out to avoid infinite looping.\n    \"\"\"\n    included \u003d list(initial_list)\n    while True:\n        changed\u003dFalse\n        # forward step\n        excluded \u003d list(set(X.columns)-set(included))\n        new_pval \u003d pd.Series(index\u003dexcluded)\n        for new_column in excluded:\n            model \u003d sm.Logit(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] \u003d model.pvalues[new_column]\n        best_pval \u003d new_pval.min()\n        if best_pval \u003c threshold_in:\n            best_feature \u003d new_pval.idxmin()\n            included.append(best_feature)\n            changed\u003dTrue\n            if verbose:\n                print(\u0027Add  {:30} with p-value {:.6}\u0027.format(best_feature, best_pval))\n\n        # backward step\n        model \u003d sm.Logit(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues \u003d model.pvalues.iloc[1:]\n        worst_pval \u003d pvalues.max() # null if pvalues is empty\n        if worst_pval \u003e threshold_out:\n            changed\u003dTrue\n            worst_feature \u003d pvalues.idxmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\u0027Drop {:30} with p-value {:.6}\u0027.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fills X (removing variables with missing values)\nselected_fields\u003ddf.drop(labels\u003d[\"id\",\"risk\"],axis\u003d1)\n# Sets y\ny \u003d df[\u0027risk\u0027]\n\nresult \u003d stepwise_selection(selected_fields, y)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\u0027resulting features:\u0027)\nprint(result)"
      ],
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Now, we run the  Model estimation only with selected features from Stepwise"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Adds selected features\nX\u003ddf\nfor item in df.columns:\n    if item not in result:\n        X\u003dX.drop(labels\u003d[item],axis\u003d1)\nX \u003d sm.add_constant(X)\n\n# statsmodels (no regularization)\nimport statsmodels.api as sm\nlogit_model\u003dsm.Logit(y,X)\nresult\u003dlogit_model.fit_regularized()\nprint(result.summary2())\n\n# print Exp(B) and confusion matrix\nparams \u003d result.params\nconf \u003d result.conf_int()\nconf[\u0027OR\u0027] \u003d params\nconf.columns \u003d [\u0027Lower\u0027, \u0027Upper\u0027, \u0027Odds Ratio\u0027]\nprint (\"\\nexp(B) \u0026 confidence intervals: \")\nprint (np.exp(conf))\n\nprint(\"\\nConfusion matrix:\")\nprint(result.pred_table())"
      ],
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Calculates AUC \u0026 optimal threshold for accuracy"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_cols \u003d X.columns[1:]\nresult \u003d sm.Logit(df[\u0027risk\u0027], df[train_cols]).fit()\ndf[\u0027pred\u0027] \u003d result.predict(df[train_cols])\nfpr, tpr, thresholds \u003droc_curve(df[\u0027risk\u0027], df[\u0027pred\u0027])\nroc_auc \u003d auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc)\n\ni \u003d np.arange(len(tpr)) # index for df\nroc \u003d pd.DataFrame({\u0027tf\u0027 : pd.Series(tpr-(1-fpr), index\u003di), \u0027cut-off\u0027 : pd.Series(thresholds, index\u003di)})\nroc_t \u003d roc.iloc[(roc.tf-0).abs().argsort()[:1]]\nprint(\"Optimal cut-off for accuracy :\")\nprint(list(roc_t[\u0027cut-off\u0027]))"
      ],
      "outputs": []
    }
  ]
}